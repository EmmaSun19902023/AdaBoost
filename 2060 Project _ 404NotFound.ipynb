{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3730737c-d286-4edd-81ba-5982ee4e78cc",
   "metadata": {},
   "source": [
    "# math and the numerical algorithms \n",
    "\n",
    "- overview, advantages and disadvantages\n",
    "- Representation, Loss, Optimizer\n",
    "- equations to explain math\n",
    "- pseudo-code to explain how numerical algorithms work\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e5a92-5799-4d01-a84d-7e2eba675616",
   "metadata": {},
   "source": [
    "# references and citations / previous work\n",
    "\n",
    "- Find at least one previous work where your ML algorithm is applied on a public dataset\n",
    "- two parts of citations and references, one in the Overview and one in the Check Model \n",
    "- Use the Harvard citation format\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7610aa3-d50b-4f0d-a8db-329ab8636378",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc21c6c6-d4ef-4333-9e8f-beee4a90269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add docstrings to each method and function and explain what they do and what the inputs and outputs are\n",
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost parameters.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []  # Weak classifiers\n",
    "        self.alphas = []  # Classifier weights\n",
    "\n",
    "    def preprocess(self, X, method='minmax'):\n",
    "        \"\"\"\n",
    "        Preprocess data using MinMaxScaler or StandardScaler.\n",
    "        \"\"\"\n",
    "        if method == 'minmax':\n",
    "            X_min = np.min(X, axis=0)\n",
    "            X_max = np.max(X, axis=0)\n",
    "            return (X - X_min) / (X_max - X_min)\n",
    "        elif method == 'standard':\n",
    "            mean = np.mean(X, axis=0)\n",
    "            std = np.std(X, axis=0)\n",
    "            return (X - mean) / std\n",
    "        else:\n",
    "            raise ValueError(\"Invalid preprocessing method. Choose 'minmax' or 'standard'.\")\n",
    "\n",
    "    def split_data(self, X, y, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Split dataset into training, validation, and test sets.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        indices = np.arange(len(y))\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        train_end = int(train_ratio * len(y))\n",
    "        val_end = train_end + int(val_ratio * len(y))\n",
    "        \n",
    "        train_idx, val_idx, test_idx = indices[:train_end], indices[train_end:val_end], indices[val_end:]\n",
    "        return X[train_idx], y[train_idx], X[val_idx], y[val_idx], X[test_idx], y[test_idx]\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        Train AdaBoost using decision stumps.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for estimator_idx in range(self.n_estimators):\n",
    "            # Train a weak classifier (decision stump)\n",
    "            stump = self._train_stump(X, y, weights)\n",
    "            pred = stump.predict(X)\n",
    "\n",
    "            # Compute error and alpha\n",
    "            error = np.sum(weights * (pred != y))\n",
    "            if error == 0:\n",
    "                error = 1e-10  # Avoid division by zero\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "            # Update weights\n",
    "            weights *= np.exp(-alpha * y * pred)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            # Store model and alpha\n",
    "            self.models.append(stump)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the trained AdaBoost model.\n",
    "        \"\"\"\n",
    "        final_pred = np.zeros(X.shape[0])\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            final_pred += alpha * model.predict(X)\n",
    "        return np.sign(final_pred)\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute misclassification error.\n",
    "        \"\"\"\n",
    "        return np.mean(y_true != y_pred)\n",
    "\n",
    "    def _train_stump(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Train a decision stump (weak classifier).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        best_stump = None\n",
    "        best_error = float('inf')\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                for polarity in [-1, 1]:\n",
    "                    stump = DecisionStump(feature_idx, threshold, polarity)\n",
    "                    pred = stump.predict(X)\n",
    "                    error = np.sum(weights * (pred != y))\n",
    "\n",
    "                    if error < best_error:\n",
    "                        best_error = error\n",
    "                        best_stump = stump\n",
    "\n",
    "        return best_stump\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self, feature_idx=None, threshold=None, polarity=1):\n",
    "        \"\"\"\n",
    "        Decision stump classifier.\n",
    "        \"\"\"\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.polarity = polarity\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using the decision stump.\n",
    "        \"\"\"\n",
    "        pred = np.ones(X.shape[0])\n",
    "        if self.polarity == 1:\n",
    "            pred[X[:, self.feature_idx] < self.threshold] = -1\n",
    "        else:\n",
    "            pred[X[:, self.feature_idx] >= self.threshold] = -1\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c5645-c4f7-49d9-9425-e95e7fc77028",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5008208-3657-43a6-a6a4-3da902f87b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.06739130434782609\n",
      "Test Loss: 0.05754614549402823\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    data = np.loadtxt('/Users/emmasun/Desktop/2060/project/spambase/spambase.data', delimiter=',')\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    y = np.where(y == 0, -1, 1)  # Convert to {-1, 1}\n",
    "\n",
    "    # Initialize AdaBoost\n",
    "    model = AdaBoost(n_estimators=50)\n",
    "\n",
    "    # Preprocess and split data\n",
    "    X_scaled = model.preprocess(X, method='standard')\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = model.split_data(X_scaled, y)\n",
    "\n",
    "    # Train the model\n",
    "    model.train(X_train, y_train)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    print(f\"Validation Loss: {model.loss(y_val, y_pred_val)}\")\n",
    "\n",
    "    # Test the model\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    print(f\"Test Loss: {model.loss(y_test, y_pred_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf56e8-0589-497b-9318-f12c1ad78550",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3085c48-b233-4eff-b6cf-20e0a6c251da",
   "metadata": {},
   "source": [
    "# Explain either as comments or in a markdown cell what the goal of each test is and/or what edge case it tests for\n",
    "\n",
    "- what the goal of each test is\n",
    "- what edge case it tests for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787343bf-fe82-4b71-8bd1-d83322fa2a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def synthetic_data():\n",
    "    \"\"\"Generate synthetic dataset for testing.\"\"\"\n",
    "    X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    return X, y\n",
    "\n",
    "@pytest.fixture\n",
    "def ada_model():\n",
    "    \"\"\"Return an instance of the AdaBoost class.\"\"\"\n",
    "    from adaboost import AdaBoost  # Import your implementation\n",
    "    return AdaBoost(n_estimators=5)\n",
    "\n",
    "def test_preprocess_minmax(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the MinMaxScaler preprocessing.\n",
    "    Goal: Ensure data is scaled to [0, 1].\n",
    "    \"\"\"\n",
    "    X, _ = synthetic_data\n",
    "    X_scaled = ada_model.preprocess(X, method='minmax')\n",
    "    assert np.allclose(np.min(X_scaled, axis=0), 0), \"Min value should be 0.\"\n",
    "    assert np.allclose(np.max(X_scaled, axis=0), 1), \"Max value should be 1.\"\n",
    "\n",
    "def test_preprocess_standard(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the StandardScaler preprocessing.\n",
    "    Goal: Ensure data has mean 0 and std 1.\n",
    "    \"\"\"\n",
    "    X, _ = synthetic_data\n",
    "    X_scaled = ada_model.preprocess(X, method='standard')\n",
    "    assert np.allclose(np.mean(X_scaled, axis=0), 0, atol=1e-6), \"Mean should be 0.\"\n",
    "    assert np.allclose(np.std(X_scaled, axis=0), 1, atol=1e-6), \"Standard deviation should be 1.\"\n",
    "\n",
    "def test_split_data(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the dataset splitting functionality.\n",
    "    Goal: Ensure correct splits and no data leakage.\n",
    "    \"\"\"\n",
    "    X, y = synthetic_data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = ada_model.split_data(X, y, 0.5, 0.25, 0.25)\n",
    "    assert len(X_train) == 2, \"Training data size mismatch.\"\n",
    "    assert len(X_val) == 1, \"Validation data size mismatch.\"\n",
    "    assert len(X_test) == 1, \"Test data size mismatch.\"\n",
    "    assert len(X_train) + len(X_val) + len(X_test) == len(X), \"Data split does not match total size.\"\n",
    "\n",
    "def test_train_basic(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the basic functionality of the train method.\n",
    "    Goal: Ensure no errors during training.\n",
    "    \"\"\"\n",
    "    X, y = synthetic_data\n",
    "    ada_model.train(X, y)\n",
    "    assert len(ada_model.models) == ada_model.n_estimators, \"Incorrect number of weak classifiers.\"\n",
    "    assert len(ada_model.alphas) == ada_model.n_estimators, \"Incorrect number of alphas.\"\n",
    "\n",
    "def test_predict_basic(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the basic functionality of the predict method.\n",
    "    Goal: Ensure predictions are correct for a trained model.\n",
    "    \"\"\"\n",
    "    X, y = synthetic_data\n",
    "    ada_model.train(X, y)\n",
    "    predictions = ada_model.predict(X)\n",
    "    assert len(predictions) == len(y), \"Number of predictions does not match input size.\"\n",
    "    assert np.all(np.isin(predictions, [-1, 1])), \"Predictions should be in {-1, 1}.\"\n",
    "\n",
    "def test_loss_basic(synthetic_data, ada_model):\n",
    "    \"\"\"\n",
    "    Test the loss calculation method.\n",
    "    Goal: Ensure loss is calculated correctly.\n",
    "    \"\"\"\n",
    "    X, y = synthetic_data\n",
    "    ada_model.train(X, y)\n",
    "    predictions = ada_model.predict(X)\n",
    "    loss = ada_model.loss(y, predictions)\n",
    "    assert 0 <= loss <= 1, \"Loss should be in [0, 1].\"\n",
    "\n",
    "def test_edge_case_uniform_data(ada_model):\n",
    "    \"\"\"\n",
    "    Test for uniform data.\n",
    "    Goal: Ensure AdaBoost handles cases where all features are the same.\n",
    "    \"\"\"\n",
    "    X = np.array([[1, 1], [1, 1], [1, 1], [1, 1]])\n",
    "    y = np.array([1, -1, 1, -1])\n",
    "    ada_model.train(X, y)\n",
    "    predictions = ada_model.predict(X)\n",
    "    assert np.all(np.isin(predictions, [-1, 1])), \"Predictions should be in {-1, 1}.\"\n",
    "    \n",
    "    \n",
    "# your implementation can correctly reproduce results \n",
    "# obtained from sklearn, textbooks, or results of peer-reviewed journal articles. \n",
    "# Previous work needs to be referenced: loss: 7%\n",
    "def test_reproduce_results(ada_model):\n",
    "    \"\"\"\n",
    "    Test reproducibility with the spambase dataset.\n",
    "    Goal: Ensure AdaBoost reproduces consistent results.\n",
    "    \"\"\"\n",
    "    data = np.loadtxt('/Users/emmasun/Desktop/2060/project/spambase/spambase.data', delimiter=',')\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "    \n",
    "    X_scaled = ada_model.preprocess(X, method='standard')\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = ada_model.split_data(X_scaled, y)\n",
    "    \n",
    "    ada_model.train(X_train, y_train)\n",
    "    predictions = ada_model.predict(X_test)\n",
    "    loss = ada_model.loss(y_test, predictions)\n",
    "    assert loss < 0.2, \"Test loss should be less than 20%.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b35cda-7688-404c-9240-892dae2f88c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a388c92-3e5c-4fcd-9e5a-8005ebb8c8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f5125-d0ba-4890-ba2d-6fc1b1c6d17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec363ee-23e4-4d79-a613-1b2cdcf5ddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4817d1f-00d8-47c1-a3a8-cc8405251896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
